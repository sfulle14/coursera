{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4: Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Design Thinking and Predictive Analytics for Data Products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earlier in this course, we learned how to to use linear regression in Python. Here we will do two examples of [Gradient Descent](https://en.wikipedia.org/wiki/Gradient_descent) using regression. The first example will feel familiar to what we have done previously, while the second one will show us how to do the same task using TensorFlow. if you dont know what Tensorflow is yet, feel free to read more [here](https://www.tensorflow.org/about)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Gradient Descent in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Data\n",
    "\n",
    "We will be looking at the `household_power_consumption.csv` file. This dataset contains roughly 2 million sample ratings of electric power consumption in one household. We are going to use this data to predict the `Kitchen power comsumption` as a function of the `Global power consumption`.\n",
    "\n",
    "Link: https://archive.ics.uci.edu/ml/datasets/Individual+household+electric+power+consumption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the Data\n",
    "\n",
    "Specify the path of the file. You may need to change the given path according to your local environment. This should be familiar to you by now, if not Homework 1 from this couse should be a good guide for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./datasets/household_power_consumption.txt\"\n",
    "f = open(path, 'r')\n",
    "\n",
    "# Note how the data is separated here\n",
    "dataset = []\n",
    "header = f.readline().strip().split(';')\n",
    "for line in f:\n",
    "    line = line.split(';')\n",
    "    dataset.append(line)\n",
    "\n",
    "header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: `Sub_metering_1` = `Kitchen power`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Sub_metering_1 = Kitchen power\n",
    "# You can read about what each column stands for by clicking on the download link for the data above\n",
    "\n",
    "# Which columns hold Global power and Kitchen power?\n",
    "print(header.index('Global_active_power'), header.index('Sub_metering_1')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice these are all strings and not numerical values!\n",
    "# How might we fix this?\n",
    "dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [d for d in dataset if d != 'NA'] # Make sure we are only using data points with values\n",
    "\n",
    "shortData = dataset[:100000] # Feel free to use a larger value here. For this example's purposes, this value will do.\n",
    "print(len(shortData), len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving our Data Format Problem\n",
    "\n",
    "In the next cell we are using two functions of Python you may not be familiar with: `map()` along with `try`/`except`. If you would like to read more about them, here are their respective links ([map](https://docs.python.org/3/library/functions.html#map)) and ([try/except](https://docs.python.org/3/tutorial/errors.html)). \n",
    "\n",
    "For now you don't have to worry about how they work, but I encourage you to play around with the for-loop and figure out what happens without the `try/except` portion. (Hint: there's a reason we have to use it here.) As for `map()`, here we are simply using it to fix the problem we had above, by getting float values out of our strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strToFloat(data):\n",
    "    data = list(map(float, data))\n",
    "    \n",
    "    \n",
    "for i in range(1,len(shortData)):\n",
    "    try:\n",
    "        strToFloat(shortData[i][2:]) # [2:] so that we only grab the numerical values, not the Time and Date\n",
    "    except ValueError:\n",
    "        shortData[i] = 'NA' # What might we be doing here?\n",
    "        \n",
    "shortData = [d for d in shortData if d != 'NA'] # This counteracts the excpet part from the for loop above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keeping It Simple\n",
    "\n",
    "For our demonstration we are only using one feature vector, `Global Power`. If you would like to predict `Kitchen Power` (or any other value) as a function of more than one feature, you would add those extra features here. How might you go about this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature(datum): # This is where the function comes from!\n",
    "    feat = [1, float(datum[2])] # Global Power\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [feature(d) for d in shortData] # Feature vector (Global Power)\n",
    "y = [float(d[6]) for d in shortData] # Kitchen Power\n",
    "K = len(X[0])\n",
    "theta = [0.0]*K\n",
    "theta[0] = sum(y) / len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are a few utility functions to compute the inner product and norm of a vector. These are also implemented in various libraries (e.g. numpy), and using the library functions would likely result in a more efficient implementation, but the functions are written here just for simplicity in our example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the inner product of a vector\n",
    "def inner(x,y):\n",
    "    return sum([a*b for (a,b) in zip(x,y)])\n",
    "\n",
    "#Defining the 2-norm of a vector\n",
    "def norm(x):\n",
    "    return sum([a*a for a in x]) # equivalently, inner(x,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Derivative Function\n",
    "\n",
    "Most important is our function to compute the derivative. The expression being computed here is the derivative of the objective (the Mean Squared Error) with respect to the parameters (`theta`), at our current estimate of `theta`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative(X, y, theta):\n",
    "    dtheta = [0.0]*len(theta) # Initialize the derivative vector to be a vector of zeros\n",
    "    K = len(theta)\n",
    "    N = len(X)\n",
    "    MSE = 0 # Compute the MSE as we go, just to print it for debugging\n",
    "    for i in range(N):\n",
    "        error = inner(X[i],theta) - y[i]\n",
    "        for k in range(K):\n",
    "            dtheta[k] += 2*X[i][k]*error/N # See lectures to understand how this expression was derived\n",
    "        MSE += error*error/N\n",
    "    return dtheta, MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's choose a Learning Rate. For this test's sake, let's call it .01. The smaller you set this, the longer your function will take to converge below, though the added computations will give a more accurate `theta` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learningRate = .01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we iteratively call our derivative function to improve our estimate of `theta`, by following the direction of the derivative. The details of this function are quite difficult to get right, e.g. if the learning rate or the convergence criteria are not set well, the function may not produce a good solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while (True):\n",
    "    dtheta,MSE = derivative(X, y, theta)\n",
    "    m = norm(dtheta)\n",
    "    print(\"norm(dtheta) = \" + str(m) + \" MSE = \" + str(MSE))\n",
    "    for k in range(K):\n",
    "        theta[k] -= learningRate * dtheta[k]\n",
    "    if m < 0.01: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Power_K = theta_0 + Power_G*theta_1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a function that determines the `Kitchen Power` as a function of the `Global Power`! Although this is acceptable it was both complicated and slow (computationally). Libraries like TensorFlow can help alleviate these issues and give the same results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Gradient Descent in Tensorflow\n",
    "\n",
    "Most of the operations will be similar so we won't explain every single function like we did above, but know that the logic is very similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf #Start by importing TensorFlow\n",
    "\n",
    "path = \"./datasets/household_power_consumption.txt\"\n",
    "f = open(path, 'r')\n",
    "\n",
    "#Read the data the same as above\n",
    "dataset = []\n",
    "header = f.readline().strip().split(';')\n",
    "for line in f:\n",
    "    line = line.split(';')\n",
    "    dataset.append(line)\n",
    "    \n",
    "header # Verify this is the same header as above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to grab however many values you want to run this on and run our code to weed out any non-numeric values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [d for d in dataset if d != 'NA'] #Make sure we are only using data points with values\n",
    "\n",
    "shortData = dataset[:100000] # Feel free to use a larger value here. For this example's purposes, this value will do\n",
    "\n",
    "def strToFloat(data):\n",
    "    data = list(map(float, data))\n",
    "    \n",
    "    \n",
    "for i in range(1,len(shortData)):\n",
    "    try:\n",
    "        strToFloat(shortData[i][2:]) # [2:] so that we only grab the numerical values, not the Time and Date\n",
    "    except ValueError:\n",
    "        shortData[i] = 'NA'\n",
    "        \n",
    "shortData = [d for d in shortData if d != 'NA'] # This counteracts the except part from the for-loop above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying Something New\n",
    "\n",
    "This time, let's add a few more features to our feature vector. Let's try calculating our `Kitchen Power` as a function of `Global Power` and `Laundry energy`. How will this change the `theta` vector we are given at the end? \n",
    "\n",
    "Note: `Sub_metering_2` = `Laundry Power`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature(datum):\n",
    "    feat = [1, float(datum[2]), float(datum[7])] # Global Active Power, Laundry Power\n",
    "    return feat\n",
    "\n",
    "X = [feature(d) for d in dataset] #Getting our feature vector as defined above\n",
    "y = [float(d[6]) for d in dataset] #Again grabbing the Kitchen power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.constant(y, shape=[len(y),1])\n",
    "K = len(XTf[0]) #same as in other example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to read what `tf.constant()` does in general, [here's](https://www.tensorflow.org/api_docs/python/tf/constant) the link. In this instance it is creating a 1-D tensor populated with the values of the list `y`.\n",
    "\n",
    "The main advantage of TensorFlow is that we don't have to compute the gradient - TensorFlow will compute it for us. Instead, we have to implement our _objective_ (i.e., the MSE) in terms of TensorFlow operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(X, y, theta):\n",
    "  return tf.reduce_mean((tf.matmul(X,theta) - y)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we tell TensorFlow that `theta` is our vector of variables to be optimized (we also specify its shape and initial values):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = tf.Variable(tf.constant([0.0]*K, shape=[K,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we select an optimizer, which is a learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we tell TensorFlow that our MSE function is the objective to be optimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective = MSE(X,y,theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we tell TensorFlow that this objective should be minimized (i.e., we are trying to minimize an error, rather than maximizing an accuracy), and initialize the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = optimizer.minimize(objective)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we run 1000 iterations of gradient descent. Note how fast this is compared to our own implementation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iteration in range(1000):\n",
    "  cvalues = sess.run([train, objective])\n",
    "  print(\"objective = \" + str(cvalues[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once gradient descent has converged, we can print out the results (i.e., `theta`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sess.as_default():\n",
    "  print(MSE(X, y, theta).eval())\n",
    "  print(theta.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a new function to determine the `Kitchen Power` consumption! It looks something like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Power_K = theta_0 + (Power_G*theta_1) + (Power_L*theta_2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go ahead and either \n",
    "    1. Change the original feature function to be a function of these two variables, OR\n",
    "    2. Change your tensorflow feature function to only be a function of Global Power\n",
    "    \n",
    "Compare the two results. The similarity between the two should serve as an indication of the power of libraries like TensorFlow and how they can aid us in solving problems like this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You're all done!\n",
    "\n",
    "You are now able to use gradient descent in two different ways! We encourage you to try this out on other datasets that interest you and see what kind of interesting data you can come up with."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
