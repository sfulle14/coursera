{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1: Supervised Learning & Regression\n",
    "#### Design Thinking and Predictive Analytics for Data Products\n",
    "\n",
    "*An Introduction to Statistical Learning* introduces supervised learning as \"building a statistical model for predicting, or estimating, an *output* based on one or more *inputs*\". This is the core difference between supervised and unsupervised learning; in unsupervised learning, there is no direct output, but we can still learn about relationships and structures in the data. As a general rule of thumb, we tend to refer to problems with a *quantitative response* as regression problems, while those involving a *qualitative response* are often referred to as classification problems. The type(s) of predictors are not usually that important when making this distinction.\n",
    "\n",
    "This week we were introduced to the concept of supervised learning. We then learned about regression and how we can do this in Python. In this notebook, we will go over basic linear regression (through library and manual approaches) and autoregression as shown in lecture videos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Linear Regression\n",
    "In the most basic form of linear regression, we assume that there is approximately a linear relationship between $x$ and $y$. In the lectures, we saw this as:\n",
    "\n",
    "$$y = x_0\\theta_0 + x_1\\theta_1 + x_2\\theta_2 + ... + x_n\\theta_n$$\n",
    "\n",
    "Linear regression is very flexible, with a million different applications! For example, you can investigate the effect of advertising budget on product sales, how much SAT scores predict university GPA, or even the relationship between teen birthrate and poverty level. Here, we will investigate some beer ratings.\n",
    "\n",
    "### The Data\n",
    "\n",
    "Unzip the `beer_50000.json` file in the Week 2 folder. This dataset contains 50,000 reviews of beers with features like the style of beer and beer taste ratings. For this notebook, we will be looking at how well a beer's overall rating can be predicted with its taste and appearance.\n",
    "\n",
    "### Reading the Data\n",
    "Specify the path of the file. You may need to change the given path according to your local environment. This should be familiar if you took Course 1 (*Basic Data Ingestion, Processing, and Visualization*) of the Python Data Products specialization already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import urllib.request # read and open URLs\n",
    "\n",
    "# Recall: the \"def\" keyword in Python defines a function.\n",
    "# What does this function do?\n",
    "def parseData(filename):\n",
    "  for line in urllib.request.urlopen(filename):\n",
    "    yield eval(line)\n",
    "\n",
    "# Process the data\n",
    "print (\"Reading data...\")\n",
    "data = list(parseData(\"file:///Users/Grace/Downloads/beer_50000.json\"))\n",
    " \n",
    "\n",
    "# Take a look at the output. Which features could be interesting to look at?\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2a: NumPy\n",
    "We will use the NumPy library to do the heavy lifting in this section, already imported above. Let's take a look at what is in our dataset. We will use the *ratings of a beer's appearance and taste* as the **features** (the variables that will predict) and a *beer's overall rating* as the **label** (what to predict)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does this function do? Complete it by filling in the brackets.\n",
    "# Why did we structure the data this way? Review the video \"Regression in Python\" for more detail.\n",
    "\n",
    "def feature(datum):\n",
    "    feat = [1, datum['review/appearance'], datum['review/taste']]\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = [feature(d) for d in data]\n",
    "y = [d['review/overall'] for d in data]\n",
    "\n",
    "# Look at first 10 rows of X and y\n",
    "print(\"Label: \", y[:10], \"\\nFeature:\", X[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's find the parameters for this linear regression model with NumPy's `lstsq` function. \n",
    "\n",
    "**History Tidbits:** *At the beginning of the nineteenth century, Legendre and Gauss published papers on the method of **least squares**, which implemented the earliest form of what is now known as linear regression. The approach was first successfully applied to problems in astronomy.*\n",
    "\n",
    "**NumPy Documentation:** https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.linalg.lstsq.html#numpy.linalg.lstsq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta,residuals,rank,s = numpy.linalg.lstsq(X, y, rcond=None)\n",
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2b: Manual Lifting\n",
    "This section is for you to better examine the linear algebra behind linear regression. Recall from the \"Supervised Learning: Regression\" lecture that the general form of a linear regression model is $$X\\theta = y$$ where we need to solve for $\\theta$. We do so by multiplying each side of the formula with the inverse of the multiplication of X-transposed and X, or $(X^TX)^{-1}$. We are left with $$\\theta = (X^TX)^{-1}X^Ty$$ as seen below. Do you get the same answer as when we performed linear regression with NumPy's provided method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = numpy.matrix(X)\n",
    "y = numpy.matrix(y)\n",
    "numpy.linalg.inv(X.T * X) * X.T * y.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what do these numbers mean? Recall that the general formula is $y = \\theta_0 + \\theta_1*X_1 + \\theta_2*X_2 + ... + \\theta_n*X_n$, where $\\theta$ represents the weights of each feature.\n",
    "\n",
    "So, you can predict that a beer's overall rating will be in the ballpark of $0.581 + 0.135 * appearance + 0.709 * taste$. You can see this with the first review in the dataset: $0.581 + 0.135(2.5) + 0.709(1.5) = 1.98$, which isn't too far off from its actual overall rating of 1.5!\n",
    "\n",
    "You can try out a few more linear regression examples on your own with the given dataset. Do earlier reviews give higher ratings? Do palate or aroma reviews matter? Is there a relationship between the length of a beer's name and the length of the reviewer's name?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Autoregression\n",
    "\n",
    "\"Autoregression is a time series model that uses *observations from previous time steps* as input to a regression equation to predict the value at the next time step.\" - [Machine Learning Mastery](\"https://machinelearningmastery.com/autoregression-models-time-series-forecasting-python/\")\n",
    "\n",
    "### The Data\n",
    "\n",
    "Unzip the `weatherHistory.csv` file in the Week 2 folder. This dataset contains over 96,000 hourly weather reports with features like pressure and humidity. For this notebook, we will be looking at how well past windspeeds can predict future windspeeds.\n",
    "\n",
    "Source: https://www.kaggle.com/muthuj7/weather-dataset/home\n",
    "\n",
    "### Reading the Data\n",
    "Specify the path of the file. You may need to change the given path according to your local environment. This should be familiar if you took Course 1 (*Basic Data Ingestion, Processing, and Visualization*) of the Python Data Products specialization already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Formatted Date',\n",
       " 'Summary',\n",
       " 'Precip Type',\n",
       " 'Temperature (C)',\n",
       " 'Apparent Temperature (C)',\n",
       " 'Humidity',\n",
       " 'Wind Speed (km/h)',\n",
       " 'Wind Bearing (degrees)',\n",
       " 'Visibility (km)',\n",
       " 'Loud Cover',\n",
       " 'Pressure (millibars)',\n",
       " 'Daily Summary']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "file = open(\"/Users/Grace/Downloads/weatherHistory.csv\", 'r')\n",
    "\n",
    "dataset = []\n",
    "header = file.readline().strip().split(',')\n",
    "for line in file:\n",
    "    line = line.split(',')\n",
    "    dataset.append(line)\n",
    "    \n",
    "header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'header' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-853a4cc86c79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Which column holds the wind speeds, the thing we are interested in?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mheader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Wind Speed (km/h)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'header' is not defined"
     ]
    }
   ],
   "source": [
    "# Which column holds the wind speeds, the thing we are interested in?\n",
    "header.index('Wind Speed (km/h)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2006-04-01 00:00:00.000 +0200',\n",
       " 'Partly Cloudy',\n",
       " 'rain',\n",
       " '9.472222222222221',\n",
       " '7.3888888888888875',\n",
       " '0.89',\n",
       " '14.1197',\n",
       " '251.0',\n",
       " '15.826300000000002',\n",
       " '0.0',\n",
       " '1015.13',\n",
       " 'Partly cloudy throughout the day.\\n']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = [d for d in dataset if d[6] != 'NA'] # get only existing values of windspeed\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our dataset, we need a way to define our moving window of values for autoregression. Let's create a function that will do this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature(dataset, index, windowSize):\n",
    "    feat = [1]\n",
    "    previousValues = [float(d[6]) for d in dataset[index - windowSize:index]]\n",
    "    return feat + previousValues\n",
    "\n",
    "# Set some constants for autoregression\n",
    "windowSize = 24;      # Window = one day\n",
    "N = len(dataset)      # defines limit of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider this line: `previousValues = [float(d[6]) for d in dataset[index - windowSize:index]]`\n",
    "\n",
    "What does `dataset[index - windowSize:index]` look like? \n",
    "\n",
    "Let's take a trivial example where `index = 20` and `windowSize = 10`. Then, we are looking at `dataset[20 - 10: 20]`, or more simply `dataset[10:20]`. Notice that the size of this chunk of dataset is exactly the window size we are looking for! Now we can use the function above to generate a \"moving window\" of values for our feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  [1, 14.1197, 14.2646, 3.9284000000000003, 14.1036, 11.0446, 13.9587, 12.3648, 14.1519, 11.3183, 12.525800000000002, 17.5651, 19.7869, 21.944300000000002, 20.6885, 15.375500000000002, 10.4006, 14.4095, 11.157300000000001, 8.5169, 7.631400000000001, 7.3899, 4.9266000000000005, 6.6493, 3.9284000000000003] \n",
      "Labels:  16.985500000000002\n"
     ]
    }
   ],
   "source": [
    "X = [feature(dataset, index, windowSize) for index in range(windowSize, N)]\n",
    "y = [float(d[6]) for d in dataset[windowSize:]]\n",
    "\n",
    "# Let's see what our predictors look like\n",
    "print(\"Features: \", X[0], \"\\nLabels: \", y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get back to NumPy and its handy-dandy linear algebra library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.04934363, -0.01724757,  0.01468831,  0.0123844 ,  0.02823784,\n",
       "       -0.00179876,  0.00827176,  0.00825164,  0.00596233, -0.00724837,\n",
       "        0.00730104, -0.00595548, -0.00703406,  0.00382461, -0.0020421 ,\n",
       "       -0.01251719,  0.00821453, -0.00772664, -0.01345461,  0.0117188 ,\n",
       "        0.00188869, -0.00534872,  0.05144874,  0.15929295,  0.66181832])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta,residuals,rank,s = numpy.linalg.lstsq(X, y, rcond=None)\n",
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the last element in this 25-dimensional array corresponds to the most recent observation in the window. So, the autoregression model looks like: $$y = 1.05 - 0.0172 * Obs_1 + 0.0147 * Obs_2 + ... + 0.662 * Obs_n$$\n",
    "\n",
    "Try out a few more autoregression examples on your own with the given dataset. What can you do with temperature, visibility, or pressure? A different window size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'Series' has no attribute 'read_csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-7373005b2b66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mseries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/Grace/Downloads/weatherHistory.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'Series' has no attribute 'read_csv'"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "from pandas import Series\n",
    "from matplotlib import pyplot\n",
    "\n",
    "series = Series.read_csv('/Users/Grace/Downloads/weatherHistory.csv', header=0)\n",
    "print (type(series))\n",
    "\n",
    "series.plot(style='d')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## You're all done!\n",
    "You should be familiar with the basics of regression in Python by now. We encourage you to explore further by using your own datasets and thinking about research questions you can answer with regression. You will have a chance to show off your regression skills at the end of this course with your very own project!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
