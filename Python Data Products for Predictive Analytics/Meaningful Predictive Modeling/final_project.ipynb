{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meaningful Predictive Modeling Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project i have used the amazon gift card review data. This data set seems to have good variaty to it and is large enough to provide accurate training and validation data sets.\n",
    "I have changed a few of the parameters from the lesson to see how they change the results. I added a few values to lambda to see if it gets better with higher or lower lambda values. I have also increased the word bank to 10,000 from 1,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "from collections import defaultdict\n",
    "import string\n",
    "import random\n",
    "from nltk.stem.porter import PorterStemmer # Stemming\n",
    "import numpy\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'amazon_reviews_us_Gift_Card_v1_00.tsv.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = gzip.open(path, 'rt', encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = f.readline()\n",
    "header = header.strip().split('\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in f:\n",
    "    fields = line.strip().split('\\t')\n",
    "    d = dict(zip(header,fields))\n",
    "    d['star_rating'] = int(d['star_rating'])\n",
    "    d['helpful_votes'] = int(d['helpful_votes'])\n",
    "    d['total_votes'] = int(d['total_votes'])\n",
    "    dataset.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCount = defaultdict(int)\n",
    "punctuation = set(string.punctuation)\n",
    "for d in dataset:\n",
    "    r = ''.join([c for c in d['review_body'].lower() if not c in punctuation])\n",
    "    for w in r.split():\n",
    "        wordCount[w] += 1\n",
    "\n",
    "counts = [(wordCount[w], w) for w in wordCount]\n",
    "counts.sort()\n",
    "counts.reverse()\n",
    "\n",
    "words = [x[1] for x in counts[:10_000]]\n",
    "\n",
    "wordId = dict(zip(words, range(len(words))))\n",
    "wordSet = set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature(datum):\n",
    "    feat = [0] * len(words)\n",
    "    r = ''.join([c for c in datum['review_body'].lower() if not c in punctuation])\n",
    "    for w in r.split():\n",
    "        if w in words:\n",
    "            feat[wordId[w]] += 1\n",
    "    feat.append(1) # offset\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [feature(d) for d in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [d['star_rating'] for d in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(X)\n",
    "X_train = X[:N//2]  # from beginning to halfway\n",
    "X_valid = X[N//2:3*N//4] # from halfway to 3/4\n",
    "X_test = X[3*N//4:] # from 3/4 to end\n",
    "\n",
    "y_train = y[:N//2]  # 50%\n",
    "y_valid = y[N//2:3*N//4] # 25%\n",
    "y_test = y[3*N//4:]  # 25%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate Mean Square Error\n",
    "def MSE(model, X, y):\n",
    "    predictions = model.predict(X)\n",
    "    differences = [(a-b)**2 for (a,b) in zip(predictions, y)]\n",
    "    return sum(differences) / len(differences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep track of which model works the best\n",
    "bestModel = None\n",
    "bestMSE = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda = 0.001, training/validation error = 0.2918175240699372/0.8186774790509076\n",
      "lambda = 0.01, training/validation error = 0.29189094015512734/0.7088928432030138\n",
      "lambda = 0.1, training/validation error = 0.29271204029047565/0.5992015854861438\n",
      "lambda = 1, training/validation error = 0.3007277044142932/0.4990569421463033\n",
      "lambda = 10, training/validation error = 0.3344172580344908/0.43391730979735676\n",
      "lambda = 100, training/validation error = 0.3872563395468438/0.4242473175453558\n",
      "lambda = 1000, training/validation error = 0.4540654698496216/0.4628953474708325\n"
     ]
    }
   ],
   "source": [
    "# fit a model for each value of lambda\n",
    "for lamb in [0.001, 0.01, 0.1, 1, 10, 100, 1000]:\n",
    "    model = linear_model.Ridge(lamb, fit_intercept=False)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    mseTrain = MSE(model, X_train, y_train)\n",
    "    mseValid = MSE(model, X_valid, y_valid)\n",
    "    \n",
    "    # report the training and validation error\n",
    "    print('lambda = ' + str(lamb) + ', training/validation error = ' +\n",
    "            str(mseTrain) + '/' + str(mseValid))\n",
    "    if not bestModel or mseValid < bestMSE:\n",
    "        bestModel = model\n",
    "        bestMSE = mseValid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test error = 0.43646316326319934\n"
     ]
    }
   ],
   "source": [
    "# Finally reprt the test error for the model with the best performance on the validation set\n",
    "mseTest = MSE(bestModel, X_test, y_test)\n",
    "print('test error = ' + str(mseTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 = 0.3730957691813146\n"
     ]
    }
   ],
   "source": [
    "FVU = mseTest / numpy.var(y_test) # Fraction of Variance Unexplained\n",
    "R2 = 1 - FVU\n",
    "print('R2 = ' + str(R2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
